{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /media/unnii/unnii/Projects/NLP/.venv/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /media/unnii/unnii/Projects/NLP/.venv/lib/python3.10/site-packages (from gensim) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /media/unnii/unnii/Projects/NLP/.venv/lib/python3.10/site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /media/unnii/unnii/Projects/NLP/.venv/lib/python3.10/site-packages (from gensim) (6.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations # For annotation purposes\n",
    "import re\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier:\n",
    "    \"\"\"\n",
    "    Class abstraction for any sentiment analyser\n",
    "    \"\"\"\n",
    "    def Train(self, data : list[tuple[str,int]]):\n",
    "        \"\"\"\n",
    "        Train the classifier.\n",
    "\n",
    "        Parameters:\n",
    "        data : Train data. Should be a list of tuples of (input paragraph, sentiment class)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def Predict(self, sentance : str) -> int:\n",
    "        \"\"\"\n",
    "        Predict class for a string\n",
    "        \"\"\"\n",
    "        pass\n",
    "            \n",
    "    def Test(self, data : list[str]) -> list[int]:\n",
    "        \"\"\"\n",
    "        Test the classifier on a list of strings. \n",
    "        Returns a same size list of ints each predicting the class of the corresponding input\n",
    "        \"\"\"\n",
    "        return [self.Predict(x) for x in data]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a\n",
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'simple',\n",
       " 'test',\n",
       " 'It',\n",
       " 'checks',\n",
       " 'if',\n",
       " 'the',\n",
       " 'function',\n",
       " 'works',\n",
       " 'properly']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ExtractWords(s : str) -> list[str]:\n",
    "    \"\"\"Extract each word from a sentance\"\"\"\n",
    "    return re.findall(r'\\b\\w+\\b', s)\n",
    "\n",
    "ExtractWords(\" Hello! How are you? This is a simple test. It checks if the function works properly!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set()\n",
    "try:\n",
    "    with open(\"stopwords.txt\") as words:\n",
    "        STOPWORDS.add(words.readline().strip())\n",
    "except FileNotFoundError:\n",
    "    STOPWORDS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bag of words as a datatype\n",
    "class BagOfWords:\n",
    "    \"\"\"\n",
    "    A bag of words, holds information about the frequency of each word in a document\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, s : str = \"\", stopwordRemoval : bool = False) -> dict[str:int]:\n",
    "        \"\"\"\n",
    "        Converts a string into a bag of words\n",
    "        \"\"\"\n",
    "        self.bow_dict = {}\n",
    "\n",
    "        for word in ExtractWords(s):\n",
    "            if stopwordRemoval and STOPWORDS:\n",
    "                if word in STOPWORDS:\n",
    "                    continue\n",
    "            if word in self.bow_dict:\n",
    "                self.bow_dict[word] += 1\n",
    "            else:\n",
    "                self.bow_dict[word] = 1\n",
    "\n",
    "    # Modifiers\n",
    "    def Add(self, s : str | BagOfWords):\n",
    "        \"\"\"\n",
    "        Add 2 bag of words or a bag of words to a string\n",
    "        \"\"\"\n",
    "        if isinstance(s, str):\n",
    "            s = BagOfWords(s)\n",
    "\n",
    "        if not isinstance(s, BagOfWords):\n",
    "            raise TypeError(\"BagOfWords can only be added to strings or BagOfWords\")\n",
    "\n",
    "        s_dict = s.GetFrequencyDict()\n",
    "        return_bow = self.copy()\n",
    "        for word in s_dict:\n",
    "            if word in self.bow_dict:\n",
    "                return_bow.bow_dict[word] += s_dict[word]\n",
    "            else:\n",
    "                return_bow.bow_dict[word] = s_dict[word]\n",
    "        \n",
    "        return return_bow\n",
    "    \n",
    "    def __add__(self, other : str | BagOfWords):\n",
    "        return self.Add(other)\n",
    "\n",
    "    def RemoveWord(self, word : str):\n",
    "        if word not in self.bow_dict:\n",
    "            raise KeyError(\"Word not in bag of words\")\n",
    "\n",
    "        del self.bow_dict[word]\n",
    "        \n",
    "    # Queries\n",
    "    def GetFrequencyDict(self) -> dict[str:int]:\n",
    "        \"\"\"\n",
    "        Return the bag of words as a frequency dict\n",
    "        \"\"\"\n",
    "        return self.bow_dict\n",
    "    \n",
    "    def GetFrequency(self, word) -> int:\n",
    "        \"\"\"\n",
    "        Return the frequency of a word\n",
    "        \"\"\"\n",
    "        if word not in self.bow_dict:\n",
    "            return 0\n",
    "        return self.bow_dict[word]\n",
    "\n",
    "    def GetWords(self) -> list:\n",
    "        \"\"\"\n",
    "        Get all types in this bag of words\n",
    "        \"\"\"\n",
    "        return list(self.bow_dict.keys())\n",
    "    \n",
    "    def WordCount(self) -> int:\n",
    "        \"\"\"\n",
    "        Total number of tokens in this bag of words\n",
    "        \"\"\"\n",
    "        return sum(self.bow_dict.values())\n",
    "    \n",
    "    # Overrides\n",
    "    def __str__(self):\n",
    "        return str(self.bow_dict)\n",
    "    \n",
    "    def copy(self) -> BagOfWords:\n",
    "        new_bow = BagOfWords()\n",
    "        new_bow.bow_dict = self.bow_dict.copy()\n",
    "        return new_bow\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier(SentimentClassifier):\n",
    "    \"\"\"\n",
    "    A Naive Bayes sentiment classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._priors = {}\n",
    "        self._likelihoods = {}\n",
    "\n",
    "        self.classes = set()\n",
    "        self.words = set()\n",
    "        self.n = 0\n",
    "\n",
    "    def Prior(self, c) -> float[0,1]:\n",
    "        \"\"\"\n",
    "        Get prior of a class c\n",
    "        \"\"\"\n",
    "        return self._priors[c]/self.n\n",
    "    \n",
    "    def Likelihood(self, d, c) -> float[0,1]:\n",
    "        \"\"\"\n",
    "        Get likelihood of a word d given a class c\n",
    "        \"\"\"\n",
    "        return self._likelihoods[c][d]\n",
    "    \n",
    "    def CalculatePriors(self, class_data : list[int]):\n",
    "        \"\"\"\n",
    "        Calculates priors\n",
    "        \"\"\"\n",
    "        # Note that the full prior is not calculated here. \n",
    "        # The values here have to be divided by total number of classes to get prior\n",
    "        # Refer function Prior for that\n",
    "        for cl in class_data:\n",
    "            self.classes.add(cl)\n",
    "\n",
    "            if cl in self._priors:\n",
    "                self._priors[cl] += 1\n",
    "            else:\n",
    "                self._priors[cl] = 1\n",
    "\n",
    "    def CalculateLikelihood(self, data : list[tuple[str,int]], add_one = True):\n",
    "        \"\"\"\n",
    "        Calculates likelihood\n",
    "\n",
    "        To be called only after calculating priors\n",
    "        \"\"\"\n",
    "        assert(self.classes)\n",
    "\n",
    "        class_documents = {}\n",
    "\n",
    "        # Creating per class super-documents\n",
    "        for sen, cl in data:\n",
    "            if cl in class_documents:\n",
    "                class_documents[cl] += sen\n",
    "            else:\n",
    "                class_documents[cl] = BagOfWords(sen)\n",
    "\n",
    "        # Entire document as a single bag of words\n",
    "        full_document = sum([bow for bow in class_documents.values()], start = BagOfWords())\n",
    "        self.words = full_document.GetWords()\n",
    "\n",
    "        for cl in self.classes:\n",
    "            word_count = class_documents[cl].WordCount()\n",
    "\n",
    "            # Smoothening\n",
    "            if add_one:\n",
    "                word_count += len(full_document.GetWords())\n",
    "\n",
    "            self._likelihoods[cl] = {}\n",
    "\n",
    "            for w in self.words:\n",
    "                # P(w/cl) = count(w, cl)/Σ count(wi, cl)\n",
    "                # With Smoothening: P(w/cl) = count(w, cl) + 1 / Σ count(wi, cl) + |V|\n",
    "                self._likelihoods[cl][w] = ( class_documents[cl].GetFrequency(w) + int(add_one) ) / word_count\n",
    "\n",
    "\n",
    "        self.class_documents = class_documents\n",
    "\n",
    "        \n",
    "    def Train(self, data : list[tuple[str,int]]):\n",
    "        \"\"\"\n",
    "        Train the Bayes classifier.\n",
    "\n",
    "        Parameters:\n",
    "        data : Train data. Should be a list of tuples of (input paragraph, sentiment class)\n",
    "        \"\"\"\n",
    "        self.n = len(data)\n",
    "\n",
    "        self.CalculatePriors([cl for _, cl in data])\n",
    "\n",
    "        self.CalculateLikelihood(data)\n",
    "\n",
    "        # PrettyPrint(self.n)\n",
    "        # PrettyPrint(self._priors)\n",
    "        # PrettyPrint(self._likelihoods)\n",
    "\n",
    "    def Predict(self, sentance : str) -> int:\n",
    "        \"\"\"\n",
    "        Predict class for a string\n",
    "        \"\"\"\n",
    "        max_class = None\n",
    "        max_class_prob = float(\"-inf\")\n",
    "\n",
    "        words = ExtractWords(sentance)\n",
    "\n",
    "        for cl in self.classes:\n",
    "            log_likelihood = 0\n",
    "            for word in words:\n",
    "                if word not in self.words:\n",
    "                    continue\n",
    "                # log(P(wi/c))\n",
    "                log_likelihood += log(self.Likelihood(word, cl))\n",
    "\n",
    "            # log(P(c/w1, w2.. wm)) = log(P(c))   +   Σ log(P(wi/c))\n",
    "            class_prob = log(self.Prior(cl)) + log_likelihood\n",
    "\n",
    "            if class_prob > max_class_prob:\n",
    "                max_class_prob = class_prob\n",
    "                max_class = cl\n",
    "\n",
    "        return max_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b\n",
    "# Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    The abstract class that the logistic regression classifier will use to extract features from a string\n",
    "    \"\"\"\n",
    "\n",
    "    def Extract(self, s : str) -> list:\n",
    "        \"\"\"\n",
    "        Extract the features from a string and return them as a list\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def Train(self, data : list[str]):\n",
    "        \"\"\"\n",
    "        Compute all features from the given dataset\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def GetFeatureCount(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns number of features\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class BagOfWordsExtractor(FeatureExtractor):\n",
    "    \"\"\"\n",
    "    A FeatureExtractor that treats the frequency of each word as a feature\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.features = []\n",
    "    \n",
    "    def Train(self, data: list[str]) -> list:\n",
    "        self.features = sum([BagOfWords(s, stopwordRemoval=True) for s in data], start=BagOfWords()).GetWords()\n",
    "    \n",
    "    def Extract(self, s: str) -> list:\n",
    "        bow = BagOfWords(s, stopwordRemoval=True)\n",
    "        features = self.features.copy()\n",
    "        for i,feature in enumerate(features):\n",
    "            features[i] = bow.GetFrequency(feature)\n",
    "\n",
    "        return features\n",
    "    \n",
    "    def GetFeatureCount(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns number of features\n",
    "        \"\"\"\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "class Word2VecExtractor(FeatureExtractor):\n",
    "    \"\"\"\n",
    "    A FeatureExtractor that uses Word2Vec embeddings from gensim\n",
    "    \"\"\"\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1, sg=0):\n",
    "        # Initialize Word2Vec parameters\n",
    "        self.vector_size = vector_size  # Dimensionality of word vectors\n",
    "        self.window = window  # Maximum distance between the current and predicted word\n",
    "        self.min_count = min_count  # Ignores all words with total frequency lower than this\n",
    "        self.sg = sg  # Training algorithm: 0 for CBOW, 1 for Skip-gram\n",
    "        self.model = None\n",
    "\n",
    "    def Train(self, data: list[str]):\n",
    "        # Train a Word2Vec model on the provided dataset\n",
    "        tokenized_data = [s.split() for s in data]\n",
    "        self.model = Word2Vec(\n",
    "            tokenized_data,\n",
    "            vector_size=self.vector_size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            sg=self.sg\n",
    "        )\n",
    "        self.model.train(tokenized_data, total_examples=len(tokenized_data), epochs=10)  # Adjust epochs as needed\n",
    "\n",
    "    def Extract(self, s):\n",
    "        \"\"\"\n",
    "        Extract Word2Vec embeddings for the words in the input string `s` and return a list of feature vectors.\n",
    "        \"\"\"\n",
    "        doc = s.split()\n",
    "        words = [word for word in doc if word in self.model.wv]\n",
    "        if not words:\n",
    "            return np.zeros(self.model.vector_size)\n",
    "        return np.mean([self.model.wv[word] for word in words], axis=0)\n",
    "\n",
    "    def GetFeatureCount(self):\n",
    "        \"\"\"\n",
    "        Returns the size of the Word2Vec embeddings, which is the length of the feature vectors.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            return 0\n",
    "        return self.model.vector_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SoftMax(c, class_scores) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the softmax of a certain class given the values for all classes\n",
    "    \"\"\"\n",
    "    return np.exp(class_scores[c], sum(np.exp(x) for x in class_scores))\n",
    "\n",
    "def SoftMaxV(class_scores : list[float]) -> np.ndarray[float]:\n",
    "    \"\"\"\n",
    "    Vectorised SoftMax\n",
    "    \"\"\"\n",
    "    exp_z = np.exp(class_scores - np.max(class_scores, axis=1, keepdims=True))\n",
    "    return exp_z / exp_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "def Sigmoid(x):\n",
    "    \"\"\"Sigmoid function. Already vectorized\"\"\"\n",
    "    return 1/1+np.exp(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier here uses cross entropy loss as objective function and performs gradient descend. Feature extraction and classification function can be provided as input to the model.\n",
    "\n",
    "Also note that feature extraction is done as a part of training in the model itself and is not done in preprocessing. This allows for increased flexibility in the model (Any arbitrary feature extraction algorithm can be used) but also adds the training overhead of the feature extractor onto the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier(SentimentClassifier):\n",
    "    \"\"\"\n",
    "    Logistic regression for multiple classes\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 featureExtractor : FeatureExtractor = BagOfWordsExtractor(), \n",
    "                 classificationFunction : callable[[list[float]], list[float]] = SoftMaxV,\n",
    "                 epochs = 1000, learningRate = 0.1):\n",
    "        \n",
    "        self.featureExtractor = featureExtractor\n",
    "        self.classificationFunction = classificationFunction\n",
    "        # self.objectiveFunction = \n",
    "        self.epochs = epochs\n",
    "        self.learningRate = learningRate\n",
    "\n",
    "        self.classes = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def GradientDescend(self, X : np.ndarray, Y : np.ndarray):\n",
    "        \"\"\"\n",
    "        Perform gradient descent on the given dataset to find weights and biases\n",
    "        \"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            # Z = X.W + B\n",
    "            Z = np.dot(X,self.weights) + self.bias\n",
    "\n",
    "            # H = σ(W) = Y_calc\n",
    "            H = self.classificationFunction(Z)\n",
    "\n",
    "            # c = (-1/M) Σ Yi log(Hi)\n",
    "            # cost = (-1/self.N)*(sum([np.dot(Y.T,np.log(H))]))     # Cross entropic loss\n",
    "\n",
    "            if not epoch%100: print(f\"[GradDesc] epochs = {epoch}\")\n",
    "\n",
    "            # Gradient = X . (Y_calc - Y_actual)\n",
    "            dcost = np.dot(X.T, (H - Y)) / self.N       # Derivative of cross entropic loss\n",
    "            \n",
    "            # Gradient descent updation\n",
    "            self.weights -= self.learningRate * dcost\n",
    "            self.bias -= self.learningRate * np.sum(H - Y, axis=0) / self.N\n",
    "\n",
    "\n",
    "    def Train(self, data: list[tuple[str, int]]):\n",
    "        self.N = len(data)\n",
    "\n",
    "        sentances, classes = list(zip(*data))\n",
    "\n",
    "        # Extract classes and features\n",
    "        self.classes = list(set(classes))\n",
    "        self.featureExtractor.Train(sentances)\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.zeros((self.featureExtractor.GetFeatureCount(), len(self.classes)))\n",
    "        self.bias = np.zeros((1, len(self.classes)))\n",
    "\n",
    "        # Initialise input and output matrises for gradient descent\n",
    "        ## Y_calc = σ(X.W + B)\n",
    "        Y = np.eye(len(self.classes))[[self.classes.index(y) for y in classes]]\n",
    "        X = np.array([self.featureExtractor.Extract(x) for x in sentances])\n",
    "\n",
    "        self.GradientDescend(X, Y)\n",
    "\n",
    "    def Predict(self, s : str):\n",
    "        x = self.featureExtractor.Extract(s)\n",
    "        z = self.classificationFunction(np.dot(x, self.weights) + self.bias)\n",
    "        return self.classes[np.argmax(z, axis=1)[0]]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrettyPrint(o, depth=0, end=\"\\n\"):\n",
    "    \"\"\"\n",
    "    For prettyprinting datatypes\n",
    "    \"\"\"\n",
    "    if isinstance(o, dict):\n",
    "        print(\"\\t\"*depth + \"{\")\n",
    "        for key in o:\n",
    "            PrettyPrint(key, depth+1, end=\":\\n\")\n",
    "            PrettyPrint(o[key], depth+2)\n",
    "        print(\"\\t\"*depth + \"}\")\n",
    "    elif isinstance(o, list):\n",
    "        print(\"\\t\"*depth + \"[\")\n",
    "        for value in o:\n",
    "            PrettyPrint(value, depth+1)\n",
    "        print(\"\\t\"*depth + \"]\")\n",
    "    elif isinstance(o, BagOfWords):\n",
    "        PrettyPrint(o.bow_dict)\n",
    "    else:\n",
    "        print(\"\\t\"*depth, end=\"\")\n",
    "        print(o, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(predicted : list, expected : list, classes : set) -> dict[dict]:\n",
    "    \"\"\"\n",
    "    Creates an nxn confusion matrix (n being the number of classes)\n",
    "\n",
    "    :param predicted: List of predicted labels\n",
    "    :param expected: List of true labels\n",
    "    :param classes: Set of unique class labels\n",
    "    :return: A 2D dictionary representing the confusion matrix\n",
    "    \"\"\"\n",
    "    confusion_matrix = {true_class: {predicted_class: 0 for predicted_class in classes} for true_class in classes}\n",
    "    \n",
    "    for true_label, predicted_label in zip(expected, predicted):\n",
    "        confusion_matrix[predicted_label][true_label] += 1\n",
    "\n",
    "    return confusion_matrix\n",
    "\n",
    "def GetPrecisions(confusion_matrix : dict[dict]) -> dict:\n",
    "    precisions = {}\n",
    "\n",
    "    for true_class in confusion_matrix:\n",
    "        precisions[true_class] = confusion_matrix[true_class][true_class] / sum( confusion_matrix[cl][true_class] for cl in confusion_matrix )\n",
    "\n",
    "    return precisions\n",
    "\n",
    "def GetRecalls(confusion_matrix : dict[dict]) -> dict:\n",
    "    recalls = {}\n",
    "\n",
    "    for true_class in confusion_matrix:\n",
    "        recalls[true_class] = confusion_matrix[true_class][true_class] / sum(confusion_matrix[true_class].values())\n",
    "\n",
    "    return recalls\n",
    "\n",
    "def GetAccuracy(confusion_matrix : dict[dict]) -> float:\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for predicted_label in confusion_matrix:\n",
    "        for true_label in confusion_matrix[predicted_label]:\n",
    "            if true_label == predicted_label:\n",
    "                correct += confusion_matrix[predicted_label][true_label]\n",
    "            total += confusion_matrix[predicted_label][true_label]\n",
    "\n",
    "    return correct/total\n",
    "\n",
    "def PrintConfusionMatrix(confusion_matrix : dict[dict]):\n",
    "    \"\"\"\n",
    "    Prints a Confusion matrix as an n x n table.\n",
    "\n",
    "    :param confusion_matrix: A 2D dictionary representing the confusion matrix\n",
    "    \"\"\"\n",
    "    LEN = 10\n",
    "\n",
    "    classes = sorted(confusion_matrix.keys())\n",
    "    \n",
    "    # Print the header row\n",
    "    header = [' '*LEN] + classes\n",
    "    header_line = \" | \".join(label.center(LEN) for label in header)\n",
    "    print(header_line)\n",
    "    print(\"-\" * len(header_line))\n",
    "    \n",
    "    for true_class in classes:\n",
    "        row = [ true_class.ljust(LEN) ] + [str(confusion_matrix[true_class][predicted_class]).center(LEN) for predicted_class in classes]\n",
    "        row_line = \" | \".join(str(cell) for cell in row)\n",
    "        print(row_line)\n",
    "\n",
    "    print()\n",
    "    print(\"Precisions:\")\n",
    "    PrettyPrint(GetPrecisions(confusion_matrix), depth=1)\n",
    "    print(\"Recalls:\")\n",
    "    PrettyPrint(GetRecalls(confusion_matrix), depth=1)\n",
    "    print(\"Accuracy:\")\n",
    "    PrettyPrint(GetAccuracy(confusion_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example\n",
    "train_filename = \"data/toy_train_1.csv\"\n",
    "test_filename = \"data/toy_test_1.csv\"\n",
    "\n",
    "with open(train_filename) as file:\n",
    "    csv_file = csv.reader(file)\n",
    "    train_data = []\n",
    "    for line in csv_file:\n",
    "        train_data.append(tuple(line))\n",
    "\n",
    "with open(test_filename) as file:\n",
    "    csv_file = csv.reader(file)\n",
    "    test_data = []\n",
    "    for line in csv_file:\n",
    "        test_data.append(tuple(line))\n",
    "\n",
    "test_sentences, test_classes = list(zip(*test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test file\n",
    "filename = \"data/cleaned_data_150k.csv\" # This version is cut down to 10k entries\n",
    "\n",
    "with open(filename) as file:\n",
    "    csv_file = csv.reader(file)\n",
    "    data = []\n",
    "    for line in csv_file:\n",
    "        data.append(tuple(line))\n",
    "\n",
    "data.pop(0) # Header\n",
    "\n",
    "n = len(data)\n",
    "\n",
    "# Train test split in 5:1 ratio\n",
    "split_fraction = 0.2\n",
    "test_data, train_data = data[:int(n*split_fraction)], data[int(n*split_fraction):]\n",
    "\n",
    "test_sentences, test_classes = list(zip(*test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier\n",
      "Initializing model\n",
      "Training model\n",
      "Training dataset size: 120000\n",
      "Training time: 50.095990896224976s\n",
      "Testing\n",
      "Training dataset size: 30000\n",
      "Testing time: 369.9198033809662s\n",
      "Confusion Matrix\n",
      "           |     0      |     1      |     2     \n",
      "-------------------------------------------------\n",
      "0          |    8642    |    829     |    273    \n",
      "1          |    626     |    8154    |    789    \n",
      "2          |    619     |    1004    |    9064   \n",
      "\n",
      "Precisions:\n",
      "\t{\n",
      "\t\t1:\n",
      "\t\t\t0.8164613998197657\n",
      "\t\t2:\n",
      "\t\t\t0.8951214694844953\n",
      "\t\t0:\n",
      "\t\t\t0.8740770709011834\n",
      "\t}\n",
      "Recalls:\n",
      "\t{\n",
      "\t\t1:\n",
      "\t\t\t0.8521266590030306\n",
      "\t\t2:\n",
      "\t\t\t0.8481332459998129\n",
      "\t\t0:\n",
      "\t\t\t0.8869047619047619\n",
      "\t}\n",
      "Accuracy:\n",
      "0.862\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "print(\"Naive Bayes Classifier\")\n",
    "print(\"Initializing model\")\n",
    "model = NaiveBayesClassifier()\n",
    "print(\"Training model\")\n",
    "start_time = time.time()\n",
    "model.Train(train_data)\n",
    "print(f\"Training dataset size: {len(train_data)}\")\n",
    "print(f\"Training time: {time.time() - start_time}s\")\n",
    "\n",
    "print(\"Testing\")\n",
    "start_time = time.time()\n",
    "predictions = model.Test(test_sentences)\n",
    "print(f\"Training dataset size: {len(test_sentences)}\")\n",
    "print(f\"Testing time: {time.time() - start_time}s\")\n",
    "\n",
    "# print(model.n)\n",
    "# PrettyPrint(model._priors)\n",
    "# print()\n",
    "# PrettyPrint(model._likelihoods)\n",
    "# [print(x.ljust(40), \"|\", y.ljust(9), \"|\", z) for x,y,z in (zip(test_sentences, test_classes, predictions))]\n",
    "\n",
    "# is_correct = [int(x == test_classes[i]) for i,x in enumerate(predictions)]\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "PrintConfusionMatrix(ConfusionMatrix(predictions, test_classes, model.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression with Word2Vec feature extraction and softmax classification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifier\n",
      "Initializing model\n",
      "Training model\n",
      "[GradDesc] epochs = 0\n",
      "[GradDesc] epochs = 100\n",
      "[GradDesc] epochs = 200\n",
      "[GradDesc] epochs = 300\n",
      "[GradDesc] epochs = 400\n",
      "[GradDesc] epochs = 500\n",
      "[GradDesc] epochs = 600\n",
      "[GradDesc] epochs = 700\n",
      "[GradDesc] epochs = 800\n",
      "[GradDesc] epochs = 900\n",
      "Training dataset size: 120000\n",
      "Training time: 282.44109439849854s\n",
      "Testing\n",
      "Training dataset size: 30000\n",
      "Testing time: 11.4303560256958s\n",
      "Confusion Matrix\n",
      "           |     0      |     1      |     2     \n",
      "-------------------------------------------------\n",
      "0          |    9049    |    988     |    567    \n",
      "1          |    388     |    7796    |    675    \n",
      "2          |    450     |    1203    |    8884   \n",
      "\n",
      "Precisions:\n",
      "\t{\n",
      "\t\t1:\n",
      "\t\t\t0.7806147992390107\n",
      "\t\t2:\n",
      "\t\t\t0.8773454473632234\n",
      "\t\t0:\n",
      "\t\t\t0.9152422372812784\n",
      "\t}\n",
      "Recalls:\n",
      "\t{\n",
      "\t\t1:\n",
      "\t\t\t0.880009030364601\n",
      "\t\t2:\n",
      "\t\t\t0.8431242289076587\n",
      "\t\t0:\n",
      "\t\t\t0.8533572236891739\n",
      "\t}\n",
      "Accuracy:\n",
      "0.8576333333333334\n"
     ]
    }
   ],
   "source": [
    "# Logistic regressiong\n",
    "\n",
    "print(\"Logistic Regression Classifier\")\n",
    "print(\"Initializing model\")\n",
    "model = LogisticRegressionClassifier( featureExtractor=Word2VecExtractor(), \n",
    "                                     epochs=1000,\n",
    "                                     learningRate=0.01)\n",
    "print(\"Training model\")\n",
    "start_time = time.time()\n",
    "model.Train(train_data)\n",
    "print(f\"Training dataset size: {len(train_data)}\")\n",
    "print(f\"Training time: {time.time() - start_time}s\")\n",
    "\n",
    "print(\"Testing\")\n",
    "start_time = time.time()\n",
    "predictions = model.Test(test_sentences)\n",
    "print(f\"Training dataset size: {len(test_sentences)}\")\n",
    "print(f\"Testing time: {time.time() - start_time}s\")\n",
    "\n",
    "# print(model.n)\n",
    "# PrettyPrint(model._priors)\n",
    "# print()\n",
    "# PrettyPrint(model._likelihoods)\n",
    "# [print(x.ljust(40), \"|\", y.ljust(9), \"|\", z) for x,y,z in (zip(test_sentences, test_classes, predictions))]\n",
    "\n",
    "# is_correct = [int(x == test_classes[i]) for i,x in enumerate(predictions)]\n",
    "print(\"Confusion Matrix\")\n",
    "PrintConfusionMatrix(ConfusionMatrix(predictions, test_classes, model.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some observations\n",
    "- It can be seen that the naive bayes model has a slightly higher accuracy got this dataset while being slower over all. However, it is to note that the logistic regression has many hyperparameters (epochs, learning rate, number of feature vectors etc) the tuning of which will likely improve the model performace considerably. Perhaps a cross validation algorithm may prove useful\n",
    "- The Naive Bayes model spends more time during training than tesing. While the logistic regression model spends most of its time training and testing in rather quick. This may likely be a difference between a generative and a discriminative model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
